declare module 'types/balance' {
	export type Balance = {
	    balance_id: string;
	    amount: number;
	    units: string;
	    purchase: string;
	};

}
declare module 'types/balanceList' {
	import { Balance } from 'types/balance';
	export type BalanceList = {
	    balances?: Array<Balance>;
	};

}
declare module 'types/hit' {
	/**
	 * Represents an identified search term in the transcript
	 */
	export type Hit = {
	    /**
	     * Value between 0 and 1 that indicates the model's relative confidence in this hit.
	     */
	    confidence: number;
	    /**
	     * Offset in seconds from the start of the audio to where the hit occurs.
	     */
	    start: number;
	    /**
	     * Offset in seconds from the start of the audio to where the hit ends.
	     */
	    end: number;
	    /**
	     * Transcript that corresponds to the time between start and end.
	     */
	    snippet: string;
	};

}
declare module 'types/search' {
	import { Hit } from 'types/hit';
	/**
	 * Search result for a transcription
	 */
	export type Search = {
	    /**
	     * Term for which Deepgram is searching.
	     */
	    query: string;
	    /**
	     * Instances of query found in transcript
	     */
	    hits: Array<Hit>;
	};

}
declare module 'types/wordBase' {
	export type WordBase = {
	    word: string;
	    start: number;
	    end: number;
	    confidence: number;
	    punctuated_word?: string;
	    speaker?: number;
	};

}
declare module 'types/channel' {
	import { Search } from 'types/search';
	import { WordBase } from 'types/wordBase';
	/**
	 * Channel of speech identified by Deepgram
	 */
	export type Channel = {
	    /**
	     * Searched terms & results
	     */
	    search?: Array<Search>;
	    alternatives: Array<{
	        /**
	         * Text of speech identified by API
	         */
	        transcript: string;
	        /**
	         * Confidence in transcript generated
	         */
	        confidence: number;
	        /**
	         * Array of words included in the transcript
	         */
	        words: Array<WordBase>;
	    }>;
	};

}
declare module 'types/createKeyOptions' {
	/**
	 * Optional options used when creating an API key
	 */
	export type CreateKeyOptions = {
	    /**
	     * Date on which the key you would like to create should expire.
	     */
	    expirationDate?: Date;
	    /**
	     * Length of time (in seconds) during which the key you would like to create will remain valid.
	     */
	    timeToLive?: number;
	};

}
declare module 'types/invitationOptions' {
	export type InvitationOptions = {
	    email?: string;
	    scope?: string;
	};

}
declare module 'types/invitationList' {
	import { InvitationOptions } from 'types/invitationOptions';
	export type InvitationList = {
	    invites?: Array<InvitationOptions>;
	};

}
declare module 'types/key' {
	/**
	 * API key used for authenticating with the Deepgram API
	 */
	export type Key = {
	    /**
	     * Unique identifier of the key to use in API requests
	     */
	    api_key_id: string;
	    /**
	     * API key to send in API requests (Only displayed when first created)
	     */
	    key?: string;
	    /**
	     * Comment for user reference
	     */
	    comment: string;
	    /**
	     * Timestamp of the date/time the key was created
	     */
	    created: string;
	    /**
	     * Array of scopes assigned to the key
	     */
	    scopes: Array<string>;
	};

}
declare module 'types/member' {
	export type Member = {
	    member_id: string;
	    first_name?: string;
	    last_name?: string;
	    scopes?: Array<string>;
	    email: string;
	};

}
declare module 'types/keyResponseObj' {
	import { Key } from 'types/key';
	import { Member } from 'types/member';
	export type KeyResponseObj = {
	    /**
	     * Optional member associated with the API key
	     */
	    member?: Member;
	    /**
	     * API key
	     */
	    api_key: Key;
	    /**
	     * Unique identifier of the key to use in API requests
	     * @deprecated This property has moved to api_key.api_key_id and will
	     * be removed in future versions.
	     */
	    api_key_id: string;
	    /**
	     * API key to send in API requests (Only displayed when first created)
	     * @deprecated This property has moved to api_key.key and will
	     * be removed in future versions.
	     */
	    key?: string;
	    /**
	     * Comment for user reference
	     * @deprecated This property has moved to api_key.comment and will
	     * be removed in future versions.
	     */
	    comment: string;
	    /**
	     * Timestamp of the date/time the key was created
	     * @deprecated This property has moved to api_key.created and will
	     * be removed in future versions.
	     */
	    created: string;
	    /**
	     * Array of scopes assigned to the key
	     * @deprecated This property has moved to api_key.scopes and will
	     * be removed in future versions.
	     */
	    scopes: Array<string>;
	};

}
declare module 'types/keyResponse' {
	import { KeyResponseObj } from 'types/keyResponseObj';
	/**
	 * Response from the Deepgram API to list keys
	 */
	export type KeyResponse = {
	    /**
	     * Array of API keys associated with the project
	     */
	    api_keys: Array<KeyResponseObj>;
	};

}
declare module 'enums/alternatives' {
	export const enum Alternatives {
	    One = "one-alternative",
	    Multiple = "multiple-alternatives"
	}

}
declare module 'enums/connectionState' {
	export enum ConnectionState {
	    CONNECTING = 0,
	    OPEN = 1,
	    CLOSING = 2,
	    CLOSED = 3
	}

}
declare module 'enums/diarization' {
	export const enum Diarization {
	    Diarized = "diarized",
	    NonDiarized = "non-diarized"
	}

}
declare module 'enums/liveTranscriptionEvents' {
	export const enum LiveTranscriptionEvents {
	    Open = "open",
	    Close = "close",
	    TranscriptReceived = "transcriptReceived",
	    Error = "error"
	}

}
declare module 'enums/models' {
	export const enum Models {
	    General = "general",
	    Meeting = "meeting",
	    PhoneCall = "phonecall"
	}

}
declare module 'enums/punctuation' {
	export const enum Punctuation {
	    NonPunctuated = "non-punctuated",
	    Punctuated = "punctuated"
	}

}
declare module 'enums/searchKind' {
	export const enum SearchKind {
	    NoSearch = "no-search",
	    WithSearch = "with-search"
	}

}
declare module 'enums/index' {
	export * from 'enums/alternatives';
	export * from 'enums/connectionState';
	export * from 'enums/diarization';
	export * from 'enums/liveTranscriptionEvents';
	export * from 'enums/models';
	export * from 'enums/punctuation';
	export * from 'enums/searchKind';

}
declare module 'types/liveTranscriptionOptions' {
	import { Models } from 'enums';
	/**
	 * Options for transcription
	 */
	export type LiveTranscriptionOptions = {
	    /**
	     * AI model used to process submitted audio.
	     * @default general
	     * @remarks Possible values are general, phonecall, meeting or a custom string
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/model
	     */
	    model?: Models | string;
	    /**
	     * Version of the model to use.
	     * @default latest
	     * @remarks latest OR <version_id>
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/version
	     */
	    version?: string;
	    /**
	     * Tier of the model to use.
	     * @default base
	     * @remarks Possible values are base or enhanced
	     * @see https://developers.deepgram.com/documentation/features/tier/
	     */
	    tier?: string;
	    /**
	     * Terms or phrases to search for in the submitted audio and replace
	     * @remarks Can send multiple instances in query string replace=this:that&replace=thisalso:thatalso. Replacing a term or phrase with nothing will remove the term or phrase from the audio transcript.
	     * @see https://developers.deepgram.com/documentation/features/replace/
	     */
	    replace?: string;
	    /**
	     * BCP-47 language tag that hints at the primary spoken language.
	     * @default en-US
	     * @remarks Possible values are en-GB, en-IN, en-NZ, en-US, es, fr, ko, pt,
	     * pt-BR, ru, tr or null
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/language
	     */
	    language?: string;
	    /**
	     * Indicates whether to add punctuation and capitalization to the transcript.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/punctuate
	     */
	    punctuate?: boolean;
	    /**
	     * Indicates whether to remove profanity from the transcript.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/profanity_filter
	     */
	    profanity_filter?: boolean;
	    /**
	     * Indicates whether to redact sensitive information, replacing redacted content with asterisks (*).
	     * @remarks Options include:
	     *  `pci`: Redacts sensitive credit card information, including credit card number, expiration date, and CVV
	     *  `numbers` (or `true)`: Aggressively redacts strings of numerals
	     *  `ssn` (*beta*): Redacts social security numbers
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/redact
	     */
	    redact?: Array<string>;
	    /**
	     * Indicates whether to recognize speaker changes. When set to true, each word
	     * in the transcript will be assigned a speaker number starting at 0.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/diarize
	     */
	    diarize?: boolean;
	    /**
	     * Indicates whether to transcribe each audio channel independently. When set
	     * to true, you will receive one transcript for each channel, which means you
	     * can apply a different model to each channel using the model parameter (e.g.,
	     * set model to general:phonecall, which applies the general model to channel
	     * 0 and the phonecall model to channel 1).
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/multichannel
	     */
	    multichannel?: boolean;
	    /**
	     * Maximum number of transcript alternatives to return. Just like a human listener,
	     * Deepgram can provide multiple possible interpretations of what it hears.
	     * @default 1
	     */
	    alternatives?: number;
	    /**
	     * Indicates whether to convert numbers from written format (e.g., one) to
	     * numerical format (e.g., 1). Deepgram can format numbers up to 999,999.
	     * @remarks Converted numbers do not include punctuation. For example,
	     * 999,999 would be transcribed as 999999.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/numerals
	     */
	    numerals?: boolean;
	    /**
	     * Terms or phrases to search for in the submitted audio. Deepgram searches
	     * for acoustic patterns in audio rather than text patterns in transcripts
	     * because we have noticed that acoustic pattern matching is more performant.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/search
	     */
	    search?: Array<string>;
	    /**
	     * Callback URL to provide if you would like your submitted audio to be
	     * processed asynchronously. When passed, Deepgram will immediately respond
	     * with a request_id. When it has finished analyzing the audio, it will send
	     * a POST request to the provided URL with an appropriate HTTP status code.
	     * @remarks You may embed basic authentication credentials in the callback URL.
	     * Only ports 80, 443, 8080, and 8443 can be used for callbacks.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/callback
	     */
	    callback?: string;
	    /**
	     * Keywords to which the model should pay particular attention to boosting
	     * or suppressing to help it understand context. Just like a human listener,
	     * Deepgram can better understand mumbled, distorted, or otherwise
	     * hard-to-decipher speech when it knows the context of the conversation.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/keywords
	     */
	    keywords?: Array<string>;
	    /**
	     * Indicates whether the streaming endpoint should send you updates to its
	     * transcription as more audio becomes available. By default, the streaming
	     * endpoint returns regular updates, which means transcription results will
	     * likely change for a period of time. You can avoid receiving these updates
	     * by setting this flag to false.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeStreamingAudio/properties/interim_results
	     */
	    interim_results?: boolean;
	    /**
	     * Indicates whether Deepgram will detect whether a speaker has finished
	     * speaking (or paused for a significant period of time, indicating the
	     * completion of an idea). When Deepgram detects an endpoint, it assumes
	     * that no additional data will improve its prediction, so it immediately
	     * finalizes the result for the processed time range and returns the
	     * transcript with a speech_final parameter set to true.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeStreamingAudio/properties/endpointing
	     */
	    endpointing?: boolean;
	    /**
	     * Length of time in milliseconds of silence that voice activation detection
	     * (VAD) will use to detect that a speaker has finished speaking. Used when
	     * endpointing is enabled. Defaults to 10 ms. Deepgram customers may configure
	     * a value between 10 ms and 500 ms; on-premise customers may remove this
	     * restriction.
	     * @default 10
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeStreamingAudio/properties/vad_turnoff
	     */
	    vad_turnoff?: number;
	    /**
	     * Expected encoding of the submitted streaming audio.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeStreamingAudio/properties/encoding
	     */
	    encoding?: string;
	    /**
	     * Number of independent audio channels contained in submitted streaming
	     * audio. Only read when a value is provided for encoding.
	     * @default 1
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeStreamingAudio/properties/channels
	     */
	    channels?: number;
	    /**
	     * Sample rate of submitted streaming audio. Required (and only read)
	     * when a value is provided for encoding.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeStreamingAudio/properties/sample_rate
	     */
	    sample_rate?: number;
	};

}
declare module 'types/liveTranscriptionResponse' {
	import { Channel } from 'types/channel';
	export type LiveTranscriptionResponse = {
	    channel_index: Array<number>;
	    duration: number;
	    start: number;
	    is_final: boolean;
	    speech_final: boolean;
	    channel: Channel;
	};

}
declare module 'types/memberList' {
	import { Member } from 'types/member';
	export type MemberList = {
	    members?: Array<Member>;
	};

}
declare module 'types/message' {
	export type Message = {
	    message?: string;
	};

}
declare module 'types/metadata' {
	export type Metadata = {
	    request_id: string;
	    transaction_key: string;
	    sha256: string;
	    created: string;
	    duration: number;
	    channels: number;
	};

}
declare module 'types/prerecordedTranscriptionOptions' {
	import { Models } from 'enums';
	/**
	 * Options for transcription
	 */
	export type PrerecordedTranscriptionOptions = {
	    /**
	     * AI model used to process submitted audio.
	     * @default general
	     * @remarks Possible values are general, phonecall, meeting or a custom string
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/model
	     */
	    model?: Models | string;
	    /**
	     * Version of the model to use.
	     * @default latest
	     * @remarks latest OR <version_id>
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/version
	     */
	    version?: string;
	    /**
	     * Tier of the model to use.
	     * @default base
	     * @remarks Possible values are base or enhanced
	     * @see https://developers.deepgram.com/documentation/features/tier/
	     */
	    tier?: string;
	    /**
	     * Terms or phrases to search for in the submitted audio and replace
	     * @remarks Can send multiple instances in query string replace=this:that&replace=thisalso:thatalso. Replacing a term or phrase with nothing will remove the term or phrase from the audio transcript.
	     * @see https://developers.deepgram.com/documentation/features/replace/
	     */
	    replace?: string;
	    /**
	     * BCP-47 language tag that hints at the primary spoken language.
	     * @default en-US
	     * @remarks Possible values are en-GB, en-IN, en-NZ, en-US, es, fr, ko, pt,
	     * pt-BR, ru, tr or null
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/language
	     */
	    language?: string;
	    /**
	     * Indicates whether to add punctuation and capitalization to the transcript.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/punctuate
	     */
	    punctuate?: boolean;
	    /**
	     * Indicates whether to remove profanity from the transcript.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/profanity_filter
	     */
	    profanity_filter?: boolean;
	    /**
	     * Indicates whether to redact sensitive information, replacing redacted content with asterisks (*).
	     * @remarks Options include:
	     *  `pci`: Redacts sensitive credit card information, including credit card number, expiration date, and CVV
	     *  `numbers` (or `true)`: Aggressively redacts strings of numerals
	     *  `ssn` (*beta*): Redacts social security numbers
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/redact
	     */
	    redact?: Array<string>;
	    /**
	     * Indicates whether to recognize speaker changes. When set to true, each word
	     * in the transcript will be assigned a speaker number starting at 0.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/diarize
	     */
	    diarize?: boolean;
	    /**
	     * Indicates whether to transcribe each audio channel independently. When set
	     * to true, you will receive one transcript for each channel, which means you
	     * can apply a different model to each channel using the model parameter (e.g.,
	     * set model to general:phonecall, which applies the general model to channel
	     * 0 and the phonecall model to channel 1).
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/multichannel
	     */
	    multichannel?: boolean;
	    /**
	     * Maximum number of transcript alternatives to return. Just like a human listener,
	     * Deepgram can provide multiple possible interpretations of what it hears.
	     * @default 1
	     */
	    alternatives?: number;
	    /**
	     * Indicates whether to convert numbers from written format (e.g., one) to
	     * numerical format (e.g., 1). Deepgram can format numbers up to 999,999.
	     * @remarks Converted numbers do not include punctuation. For example,
	     * 999,999 would be transcribed as 999999.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/numerals
	     */
	    numerals?: boolean;
	    /**
	     * Terms or phrases to search for in the submitted audio. Deepgram searches
	     * for acoustic patterns in audio rather than text patterns in transcripts
	     * because we have noticed that acoustic pattern matching is more performant.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/search
	     */
	    search?: Array<string>;
	    /**
	     * Callback URL to provide if you would like your submitted audio to be
	     * processed asynchronously. When passed, Deepgram will immediately respond
	     * with a request_id. When it has finished analyzing the audio, it will send
	     * a POST request to the provided URL with an appropriate HTTP status code.
	     * @remarks You may embed basic authentication credentials in the callback URL.
	     * Only ports 80, 443, 8080, and 8443 can be used for callbacks.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/callback
	     */
	    callback?: string;
	    /**
	     * Keywords to which the model should pay particular attention to boosting
	     * or suppressing to help it understand context. Just like a human listener,
	     * Deepgram can better understand mumbled, distorted, or otherwise
	     * hard-to-decipher speech when it knows the context of the conversation.
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/keywords
	     */
	    keywords?: Array<string>;
	    /**
	     * Indicates whether Deepgram will segment speech into meaningful semantic
	     * units, which allows the model to interact more naturally and effectively
	     * with speakers' spontaneous speech patterns. For example, when humans
	     * speak to each other conversationally, they often pause mid-sentence to
	     * reformulate their thoughts, or stop and restart a badly-worded sentence.
	     * When utterances is set to true, these utterances are identified and
	     * returned in the transcript results.
	     *
	     * By default, when utterances is enabled, it starts a new utterance after
	     * 0.8 s of silence. You can customize the length of time used to determine
	     * where to split utterances by submitting the utt_split parameter.
	     * @remarks **BETA FEATURE**
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/utterances
	     */
	    utterances?: boolean;
	    /**
	     * Length of time in seconds of silence between words that Deepgram will
	     * use when determining where to split utterances. Used when utterances
	     * is enabled.
	     * @default 0.8 seconds
	     * @remarks **BETA FEATURE**
	     * @see https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/utt_split
	     */
	    utt_split?: number;
	};

}
declare module 'types/utterance' {
	import { WordBase } from 'types/wordBase';
	export type Utterance = {
	    /**
	     * Start time (in seconds) from the beginning of the audio stream.
	     */
	    start: number;
	    /**
	     * End time (in seconds) from the beginning of the audio stream.
	     */
	    end: number;
	    /**
	     * Floating point value between 0 and 1 that indicates overall transcript
	     * reliability. Larger values indicate higher confidence.
	     */
	    confidence: number;
	    /**
	     * Audio channel to which the utterance belongs. When using multichannel audio,
	     * utterances are chronologically ordered by channel.
	     */
	    channel: number;
	    /**
	     *  Transcript for the audio segment being processed.
	     */
	    transcript: string;
	    /**
	     * Object containing each word in the transcript, along with its start time
	     * and end time (in seconds) from the beginning of the audio stream, and a confidence value.
	     */
	    words: Array<WordBase>;
	    /**
	     * Integer indicating the predicted speaker of the majority of words
	     * in the utterance who is saying the words being processed.
	     */
	    speaker?: number;
	    /**
	     * Unique identifier of the utterance
	     */
	    id: string;
	};

}
declare module 'helpers/secondsToTimestamp' {
	export function secondsToTimestamp(seconds: number): string;

}
declare module 'helpers/validateOptions' {
	export function validateOptions(apiKey: string, apiUrl: string): void;

}
declare module 'helpers/index' {
	export * from 'helpers/secondsToTimestamp';
	export * from 'helpers/validateOptions';

}
declare module 'types/prerecordedTranscriptionResponse' {
	import { Metadata } from 'types/metadata';
	import { Channel } from 'types/channel';
	import { Utterance } from 'types/utterance';
	export class PrerecordedTranscriptionResponse {
	    request_id?: string;
	    metadata?: Metadata;
	    results?: {
	        channels: Array<Channel>;
	        utterances?: Array<Utterance>;
	    };
	    /**
	     * Converts the transcription to the WebVTT format
	     * @remarks In order to translate the transcription to WebVTT, the utterances
	     * feature must be used.
	     * @returns A string with the transcription in the WebVTT format
	     */
	    toWebVTT(): string;
	    /**
	     * Converts the transcription to the SRT format
	     * @remarks In order to translate the transcription to SRT, the utterances
	     * feature must be used.
	     * @returns A string with the transcription in the SRT format
	     */
	    toSRT(): string;
	}

}
declare module 'types/project' {
	/**
	 * Deepgram project
	 */
	export type Project = {
	    /**
	     * Unique identifier of the project
	     */
	    project_id: string;
	    /**
	     * User provided name of the project
	     */
	    name?: string;
	    /**
	     * Name of the company associated with the project. Optional.
	     */
	    company?: string;
	};

}
declare module 'types/projectPatchResponse' {
	export type ProjectPatchResponse = {
	    /**
	     * Success message.
	     */
	    message: string;
	};

}
declare module 'types/projectResponse' {
	import { Project } from 'types/project';
	export type ProjectResponse = {
	    projects: Array<Project>;
	};

}
declare module 'types/scopeList' {
	export type ScopeList = {
	    scopes: Array<string>;
	};

}
declare module 'types/transcriptionSource' {
	/// <reference types="node" />
	import { ReadStream } from 'fs';
	export type TranscriptionSource = UrlSource | BufferSource | ReadStreamSource;
	export type ReadStreamSource = {
	    stream: ReadStream;
	    mimetype: string;
	};
	export type UrlSource = {
	    url: string;
	};
	export type BufferSource = {
	    buffer: Buffer;
	    mimetype: string;
	};

}
declare module 'types/usageCallback' {
	export type UsageCallback = {
	    code: number;
	    completed: string;
	};

}
declare module 'types/usageField' {
	export type UsageField = {
	    tags: Array<string>;
	    models: Array<string>;
	    processing_methods: Array<string>;
	    languages: Array<string>;
	    features: Array<string>;
	};

}
declare module 'types/usageFieldOptions' {
	export type UsageFieldOptions = {
	    start?: string;
	    end?: string;
	};

}
declare module 'types/usageOptions' {
	export type UsageOptions = {
	    start?: string;
	    end?: string;
	    accessor?: string;
	    tag?: Array<string>;
	    method?: "sync" | "async" | "streaming";
	    model?: string;
	    multichannel?: boolean;
	    interim_results?: boolean;
	    punctuate?: boolean;
	    ner?: boolean;
	    utterances?: boolean;
	    replace?: boolean;
	    profanity_filter?: boolean;
	    keywords?: boolean;
	    sentiment?: boolean;
	    diarize?: boolean;
	    detect_language?: boolean;
	    search?: boolean;
	    redact?: boolean;
	    alternatives?: boolean;
	    numerals?: boolean;
	};

}
declare module 'types/usageRequestDetail' {
	export type UsageRequestDetail = {
	    details: {
	        usd: number;
	        duration: number;
	        total_audio: number;
	        channels: number;
	        streams: number;
	        model: string;
	        method: "sync" | "async" | "streaming";
	        tags: Array<string>;
	        features: Array<string>;
	        config: {
	            multichannel?: boolean;
	            interim_results?: boolean;
	            punctuate?: boolean;
	            ner?: boolean;
	            utterances?: boolean;
	            replace?: boolean;
	            profanity_filter?: boolean;
	            keywords?: boolean;
	            sentiment?: boolean;
	            diarize?: boolean;
	            detect_language?: boolean;
	            search?: boolean;
	            redact?: boolean;
	            alternatives?: boolean;
	            numerals?: boolean;
	        };
	    };
	};

}
declare module 'types/usageRequestMessage' {
	export type UsageRequestMessage = {
	    message?: string;
	};

}
declare module 'types/usageRequest' {
	import { UsageCallback } from 'types/usageCallback';
	import { UsageRequestDetail } from 'types/usageRequestDetail';
	import { UsageRequestMessage } from 'types/usageRequestMessage';
	export type UsageRequest = {
	    request_id: string;
	    created: string;
	    path: string;
	    accessor: string;
	    response?: UsageRequestDetail | UsageRequestMessage;
	    callback?: UsageCallback;
	};

}
declare module 'types/usageRequestList' {
	import { UsageRequest } from 'types/usageRequest';
	export type UsageRequestList = {
	    page: number;
	    limit: number;
	    requests?: Array<UsageRequest>;
	};

}
declare module 'types/usageRequestListOptions' {
	export type UsageRequestListOptions = {
	    start?: string;
	    end?: string;
	    page?: number;
	    limit?: number;
	    status?: "succeeded" | "failed";
	};

}
declare module 'types/usageResponseDetail' {
	export type UsageResponseDetail = {
	    start: string;
	    end: string;
	    hours: number;
	    requests: number;
	};

}
declare module 'types/usageResponse' {
	import { UsageResponseDetail } from 'types/usageResponseDetail';
	export type UsageResponse = {
	    start: string;
	    end: string;
	    resolution: {
	        units: string;
	        amount: number;
	    };
	    results: Array<UsageResponseDetail>;
	};

}
declare module 'types/projectPatchRequest' {
	export type ProjectPatchRequest = {
	    name?: string;
	    company?: string;
	};

}
declare module 'types/requestFunction' {
	/// <reference types="node" />
	import { ReadStream } from 'fs';
	export type RequestFunction = NodeRequest | BrowserRequest;
	export type NodeRequest = (method: string, api_key: string, apiUrl: string, path: string, payload?: string | Buffer | ReadStream, options?: Object) => Promise<any>;
	export type BrowserRequest = (method: string, api_key: string, apiUrl: string, path: string, payload?: string) => Promise<any>;

}
declare module 'types/index' {
	export * from 'types/balance';
	export * from 'types/balanceList';
	export * from 'types/channel';
	export * from 'types/createKeyOptions';
	export * from 'types/hit';
	export * from 'types/invitationList';
	export * from 'types/invitationOptions';
	export * from 'types/key';
	export * from 'types/keyResponse';
	export * from 'types/liveTranscriptionOptions';
	export * from 'types/liveTranscriptionResponse';
	export * from 'types/member';
	export * from 'types/memberList';
	export * from 'types/message';
	export * from 'types/metadata';
	export * from 'types/prerecordedTranscriptionOptions';
	export * from 'types/prerecordedTranscriptionResponse';
	export * from 'types/project';
	export * from 'types/projectPatchResponse';
	export * from 'types/projectResponse';
	export * from 'types/scopeList';
	export * from 'types/search';
	export * from 'types/transcriptionSource';
	export * from 'types/usageCallback';
	export * from 'types/usageField';
	export * from 'types/usageFieldOptions';
	export * from 'types/usageOptions';
	export * from 'types/usageRequest';
	export * from 'types/usageRequestDetail';
	export * from 'types/usageRequestList';
	export * from 'types/usageRequestListOptions';
	export * from 'types/usageResponse';
	export * from 'types/usageResponseDetail';
	export * from 'types/utterance';
	export * from 'types/wordBase';
	export * from 'types/keyResponseObj';
	export * from 'types/projectPatchRequest';
	export * from 'types/requestFunction';

}
declare module 'billing' {
	import { BalanceList, Balance, RequestFunction } from 'types';
	export class Billing {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Retrieves list of balance info of the specified project.
	     * @param projectId Unique identifier of the project
	     */
	    listBalances(projectId: string): Promise<BalanceList>;
	    /**
	     * Retrieves balance info of a specified balance_id in the specified project.
	     * @param projectId Unique identifier of the project
	     * @param balanceId Unique identifier of the balance
	     */
	    getBalance(projectId: string, balanceId: string): Promise<Balance>;
	}

}
declare module 'userAgent' {
	export function userAgent(): string;

}
declare module 'httpRequest' {
	/// <reference types="node" />
	import { ReadStream } from 'fs';
	export function _request<T>(method: string, api_key: string, apiUrl: string, path: string, payload?: string | Buffer | ReadStream, options?: Object): Promise<T>;

}
declare module 'constants/defaultOptions' {
	/**
	 * Default SDK options
	 */
	export const DefaultOptions: {
	    apiUrl: string;
	};

}
declare module 'constants/index' {
	export * from 'constants/defaultOptions';

}
declare module 'keys' {
	import { CreateKeyOptions, KeyResponse, Key, RequestFunction } from 'types';
	export class Keys {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Retrieves all keys associated with the provided projectId
	     * @param projectId Unique identifier of the project containing API keys
	     */
	    list(projectId: string): Promise<KeyResponse>;
	    /**
	     * Retrieves a specific key associated with the provided projectId
	     * @param projectId Unique identifier of the project containing API keys
	     * @param keyId Unique identifier for the key to retrieve
	     */
	    get(projectId: string, keyId: string): Promise<Key>;
	    /**
	     * Creates an API key with the provided scopes
	     * @param projectId Unique identifier of the project to create an API key under
	     * @param comment Comment to describe the key
	     * @param scopes Permission scopes associated with the API key
	     * @param options Optional options used when creating API keys
	     */
	    create(projectId: string, comment: string, scopes: Array<string>, options?: CreateKeyOptions): Promise<Key>;
	    /**
	     * Deletes an API key
	     * @param projectId Unique identifier of the project to create an API key under
	     * @param keyId Unique identifier for the key to delete
	     */
	    delete(projectId: string, keyId: string): Promise<void>;
	}

}
declare module 'projects' {
	import { Project, ProjectPatchResponse, ProjectResponse, ProjectPatchRequest, RequestFunction } from 'types';
	export class Projects {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Returns all projects accessible by the API key
	     */
	    list(): Promise<ProjectResponse>;
	    /**
	     * Retrieves a specific project based on the provided projectId
	     * @param projectId Unique identifier of the project to retrieve
	     */
	    get(projectId: string): Promise<Project>;
	    /**
	     * Update a specific project
	     * @param project project to update
	     */
	    update(project: Project, payload: ProjectPatchRequest): Promise<ProjectPatchResponse>;
	}

}
declare module 'transcription/liveTranscription' {
	/// <reference types="node" />
	import EventEmitter from 'events';
	import { ConnectionState } from 'enums';
	import { LiveTranscriptionOptions } from 'types';
	export class LiveTranscription extends EventEmitter {
	    private _socket;
	    constructor(credentials: string, apiUrl: string, options?: LiveTranscriptionOptions);
	    private _bindSocketEvents;
	    /**
	     * Returns the ready state of the websocket connection
	     */
	    getReadyState(): ConnectionState;
	    /**
	     * Sends data to the Deepgram API via websocket connection
	     * @param data Audio data to send to Deepgram
	     */
	    send(data: string | ArrayBufferLike | Blob | ArrayBufferView): void;
	    /**
	     * Denote that you are finished sending audio and close
	     * the websocket connection when transcription is finished
	     */
	    finish(): void;
	}

}
declare module 'transcription/preRecordedTranscription' {
	import { PrerecordedTranscriptionOptions, PrerecordedTranscriptionResponse, TranscriptionSource } from 'types';
	/**
	 * Transcribes audio from a file or buffer
	 * @param credentials Base64 encoded API key & secret
	 * @param source Url or Buffer of file to transcribe
	 * @param options Options to modify transcriptions
	 */
	export const preRecordedTranscription: (apiKey: string, apiUrl: string, source: TranscriptionSource, options?: PrerecordedTranscriptionOptions | undefined) => Promise<PrerecordedTranscriptionResponse>;

}
declare module 'transcription/index' {
	import { LiveTranscriptionOptions, PrerecordedTranscriptionOptions, PrerecordedTranscriptionResponse, TranscriptionSource } from 'types';
	import { LiveTranscription } from 'transcription/liveTranscription';
	export class Transcriber {
	    private _credentials;
	    private _apiUrl;
	    constructor(_credentials: string, _apiUrl: string);
	    /**
	     * Transcribes prerecorded audio from a file or buffer
	     * @param source Url or Buffer of file to transcribe
	     * @param options Options to modify transcriptions
	     */
	    preRecorded(source: TranscriptionSource, options?: PrerecordedTranscriptionOptions): Promise<PrerecordedTranscriptionResponse>;
	    /**
	     * Opens a websocket to Deepgram's API for live transcriptions
	     * @param options Options to modify transcriptions
	     */
	    live(options?: LiveTranscriptionOptions): LiveTranscription;
	}

}
declare module 'usage' {
	import { RequestFunction, UsageField, UsageFieldOptions, UsageOptions, UsageRequest, UsageRequestList, UsageRequestListOptions, UsageResponse } from 'types';
	export class Usage {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Retrieves all requests associated with the provided projectId based
	     * on the provided options
	     * @param projectId Unique identifier of the project
	     * @param options Additional filter options
	     */
	    listRequests(projectId: string, options?: UsageRequestListOptions): Promise<UsageRequestList>;
	    /**
	     * Retrieves a specific request associated with the provided projectId
	     * @param projectId Unique identifier of the project
	     * @param requestId Unique identifier for the request to retrieve
	     */
	    getRequest(projectId: string, requestId: string): Promise<UsageRequest>;
	    /**
	     * Retrieves usage associated with the provided projectId based
	     * on the provided options
	     * @param projectId Unique identifier of the project
	     * @param options Options to filter usage
	     */
	    getUsage(projectId: string, options?: UsageOptions): Promise<UsageResponse>;
	    /**
	     * Retrieves features used by the provided projectId based
	     * on the provided options
	     * @param projectId Unique identifier of the project
	     * @param options Options to filter usage
	     */
	    getFields(projectId: string, options?: UsageFieldOptions): Promise<UsageField>;
	}

}
declare module 'members' {
	import { MemberList, Message, RequestFunction } from 'types';
	export class Members {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Retrieves account objects for all of the accounts in the specified project.
	     * @param projectId Unique identifier of the project
	     */
	    listMembers(projectId: string): Promise<MemberList>;
	    /**
	     * Retrieves account objects for all of the accounts in the specified project.
	     * @param projectId Unique identifier of the project
	     * @param memberId Unique identifier of the member
	     */
	    removeMember(projectId: string, memberId: string): Promise<Message>;
	}

}
declare module 'invitation' {
	import { Message, InvitationOptions, InvitationList, RequestFunction } from 'types';
	export class Invitation {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Lists all the current invites of a specified project.
	     * @param projectId Unique identifier of the project
	     */
	    list(projectId: string): Promise<InvitationList>;
	    /**
	     * Sends an invitation to join the specified project.
	     * @param projectId Unique identifier of the project
	     */
	    send(projectId: string, options: InvitationOptions): Promise<Message>;
	    /**
	     * Removes the authenticated account from the specified project.
	     * @param projectId Unique identifier of the project
	     */
	    leave(projectId: string): Promise<Message>;
	    /**
	     * Removes the specified email from the invitations on the specified project.
	     * @param projectId Unique identifier of the project
	     * @param email email address of the invitee
	     * NOTE: This will return successful even if the email does not have an invite on the project.
	     */
	    delete(projectId: string, email: string): Promise<Message>;
	}

}
declare module 'scopes' {
	import { ScopeList, Message, RequestFunction } from 'types';
	export class Scopes {
	    private _credentials;
	    private _apiUrl;
	    private _request;
	    constructor(_credentials: string, _apiUrl: string, _request: RequestFunction);
	    private apiPath;
	    /**
	     * Retrieves scopes of the specified member in the specified project.
	     * @param projectId Unique identifier of the project
	     * @param memberId Unique identifier of the member
	     */
	    get(projectId: string, memberId: string): Promise<ScopeList>;
	    /**
	     * Updates the scope for the specified member in the specified project.
	     * @param projectId Unique identifier of the project
	     * @param memberId Unique identifier of the member being updated
	     * @param scope string of the scope to update to
	     */
	    update(projectID: string, memberId: string, scope: string): Promise<Message>;
	}

}
declare module 'index' {
	import { Keys } from 'keys';
	import { Projects } from 'projects';
	import { Transcriber } from 'transcription';
	import { Usage } from 'usage';
	import { Members } from 'members';
	import { Invitation } from 'invitation';
	import { Billing } from 'billing';
	import { Scopes } from 'scopes';
	export class Deepgram {
	    private _apiUrl;
	    private _apiKey;
	    keys: Keys;
	    projects: Projects;
	    transcription: Transcriber;
	    usage: Usage;
	    members: Members;
	    invitation: Invitation;
	    billing: Billing;
	    scopes: Scopes;
	    constructor(apiKey: string, apiUrl?: string);
	}

}
declare module 'browser/httpFetch' {
	export function _request<T>(method: string, api_key: string, apiUrl: string, path: string, payload?: string): Promise<T>;

}
declare module 'browser/transcription/preRecordedTranscription' {
	import { PrerecordedTranscriptionOptions, PrerecordedTranscriptionResponse, UrlSource } from 'types';
	/**
	 * Transcribes audio from a url
	 * @param credentials Base64 encoded API key & secret
	 * @param apiUrl url string of Deepgram's API
	 * @param source Url or Buffer of file to transcribe
	 * @param options Options to modify transcriptions
	 */
	export const preRecordedTranscription: (apiKey: string, apiUrl: string, source: UrlSource, options?: PrerecordedTranscriptionOptions | undefined) => Promise<PrerecordedTranscriptionResponse>;

}
declare module 'browser/transcription/index' {
	import { LiveTranscriptionOptions, PrerecordedTranscriptionOptions, PrerecordedTranscriptionResponse, UrlSource } from 'types';
	export class Transcriber {
	    private _credentials;
	    private _apiUrl;
	    constructor(_credentials: string, _apiUrl: string);
	    /**
	     * Transcribes prerecorded audio from a file or buffer
	     * @param source Url or Buffer of file to transcribe
	     * @param options Options to modify transcriptions
	     */
	    preRecorded(source: UrlSource, options?: PrerecordedTranscriptionOptions): Promise<PrerecordedTranscriptionResponse>;
	    /**
	     * Opens a websocket to Deepgram's API for live transcriptions
	     * @param options Options to modify transcriptions
	     */
	    live(options?: LiveTranscriptionOptions): WebSocket;
	}

}
declare module 'browser/index' {
	import { Transcriber } from 'browser/transcription';
	import { Projects } from 'projects';
	import { Keys } from 'keys';
	import { Usage } from 'usage';
	import { Members } from 'members';
	import { Invitation } from 'invitation';
	import { Billing } from 'billing';
	import { Scopes } from 'scopes';
	export class Deepgram {
	    private _apiUrl;
	    private _apiKey;
	    transcription: Transcriber;
	    projects: Projects;
	    keys: Keys;
	    usage: Usage;
	    members: Members;
	    invitation: Invitation;
	    billing: Billing;
	    scopes: Scopes;
	    constructor(apiKey: string, apiUrl?: string);
	}

}
declare module 'types/keyword' {
	export type Keyword = {
	    keyword: string;
	    boost?: number;
	};

}
